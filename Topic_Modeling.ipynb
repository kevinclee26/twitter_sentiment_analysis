{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling\n",
    "* Input is a document-term matrix - each topic consist of a set of words where order doesn';t matter, so we're going to start with the bag of words format\n",
    "* genism is a Python toolkit built for topic modeling. \n",
    "* Latent Dirichlet Allocation (LDA) is a popular topic modeling technique. \n",
    "    * Probability distribution of topics\n",
    "    * Probability distribution of terms\n",
    "* Latent -> Hidden\n",
    "* Dirichlet -> Type of Probability Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "* Use LDA to learn the topic mix in each document, and the word mix in each topic\n",
    "1. Choose the number of topics in the corpus, K.\n",
    "2. Randomly assign each word in each document to one of K topics. \n",
    "3. Go through every word and its topic assignment in each document. Look at (1) how often the topic occurs in the document and (2) how often the word occurs in the topic overall. Based on this info, decided if the topic is appropriate or should be reassigned. \n",
    "4. Go through multiple iterations of this. Eventually, the topics will start making sense - interpret them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "* gensim takes in the data as spare matrix, which is a more efficient way to reading in text data\n",
    "* gensim takes in term document frequency\n",
    "* one popular trick is to look only at terms that are from one part of speech (only nouns, only adjuectives, etc.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ffce9913b0c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msparse_counts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtdm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# tdm is term document matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparse2Corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# cv is a pickled count vectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tdm' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "sparse_counts=scipy.sparse.csr_matrix(tdm) # tdm is term document matrix\n",
    "corpus=matutils.Sparse2Corpus(sparse_counts)\n",
    "id2word=dict((v, k) for k, v in cv.vocabulary_.items()) # cv is a pickled count vectorizer\n",
    "lda=models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()\n",
    "\n",
    "# check which topics are included in each document\n",
    "corpus_transformed=lda[corpus]\n",
    "list(zip([a for [(a, b)] in corpus_transformed], tdm.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatives\n",
    "* Matrix Factorization techniques for topic modeling such as Latent Semantic Indexing (LSI) and Non-Negative Matrix Factorization (NMF). \n",
    "* All the topic modeling are unsupervised - there are ways to use supervised for topic modeling if they have been labeled. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
